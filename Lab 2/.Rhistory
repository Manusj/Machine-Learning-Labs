Ridge <- function(params, lambda){
sigma <- params[1]
theta <- params[-1]
negativemodel_loglikelihood <- -(loglikelihood(sigma, theta))
return (negativemodel_loglikelihood+(lambda * (sum(theta^2))))
}
RidgeOpt <- function(lambda, thetha, sigma)
{
optimization_variables <- c(sigma, thetha)
optim_par <- optim(par = optimization_variables,
fn = Ridge,
lambda =lambda,
method="BFGS")
return (optim_par$par[-1])
}
DF <- function(lambda){
x <- as.matrix(train_dataset[,7:22])
P <- x%*%(solve((t(x) %*% x)+lambda))%*%t(x)
print(sum(diag(P)))
}
intial_weights = coefficients(linearRegression_model)
initial_dispersion = sd(residuals(linearRegression_model))
optimisedWeight_lambda1 = t(as.matrix(RidgeOpt(1, intial_weights, initial_dispersion)))
optimisedWeight_lambda100 = t(as.matrix(RidgeOpt(100, intial_weights, initial_dispersion)))
optimisedWeight_lambda1000 = t(as.matrix(RidgeOpt(1000, intial_weights, initial_dispersion)))
train_data_martix = t(as.matrix(train_dataset[,7:22]))
test_data_matrix = t(as.matrix(test_dataset[,7:22]))
predictions_lambda1_traindata = (optimisedWeight_lambda1 %*% train_data_martix)
predictions_lambda1_testdata = (optimisedWeight_lambda1 %*% test_data_matrix)
predictions_lambda100_traindata = (optimisedWeight_lambda100 %*% train_data_martix)
predictions_lambda100_testdata = (optimisedWeight_lambda100 %*% test_data_matrix)
predictions_lambda1000_traindata = (optimisedWeight_lambda1000 %*% train_data_martix)
predictions_lambda1000_testdata = (optimisedWeight_lambda1000 %*% test_data_matrix)
trainingMse_lambda1 = mean((train_dataset$motor_UPDRS - predictions_lambda1_traindata) ^ 2)
testMse_lambda1 = mean((test_dataset$motor_UPDRS - predictions_lambda1_testdata) ^ 2)
trainingMse_lambda100 = mean((train_dataset$motor_UPDRS - predictions_lambda100_traindata) ^ 2)
testMse_lambda100 = mean((test_dataset$motor_UPDRS - predictions_lambda100_testdata) ^ 2)
trainingMse_lambda1000 = mean((train_dataset$motor_UPDRS - predictions_lambda1000_traindata) ^ 2)
testMse_lambda1000 = mean((test_dataset$motor_UPDRS - predictions_lambda1000_testdata) ^ 2)
print(train_mse)
print(test_mse)
print(trainingMse_lambda1)
print(testMse_lambda1)
DF(1)
print(trainingMse_lambda100)
print(testMse_lambda100)
DF(100)
print(trainingMse_lambda1000)
print(testMse_lambda1000)
DF(1000)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
View(test_dataset)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
dataset_parkinsons <- scale(dataset_parkinsons)
# Splitting scaled data to 60% training and 40% test
n <- dim(dataset_parkinsons)[1]
set.seed(12345)
id <- sample(1:n, floor(n*0.6))
train_dataset <- data.frame(dataset_parkinsons[id,])
test_dataset <- data.frame(dataset_parkinsons[-id,])
# Part 2 of Assignment 2
# Creating a linear regression model with all voice characteristics as parameters and motor_UPDRS as the output
linearRegression_model <- lm(motor_UPDRS ~ 0+Jitter...+Jitter.Abs.+Jitter.RAP+Jitter.PPQ5+Jitter.DDP+Shimmer+Shimmer.dB.+Shimmer.APQ3+Shimmer.APQ5 +Shimmer.APQ11+Shimmer.DDA+NHR+HNR+RPDE+DFA+PPE,data = train_dataset)
summary(linearRegression_model)
# calculating Mean Squared error of train and test sets
testset_predictions <- predict(linearRegression_model, test_dataset)
train_mse <- mean((train_dataset$motor_UPDRS - predict(linearRegression_model)) ^ 2)
test_mse <- mean((test_dataset$motor_UPDRS - testset_predictions) ^ 2)
print(train_mse)
loglikelihood <- function(sigma, theta){
n <- dim(train_dataset)[1]
theta <- t(as.matrix(theta))
input_data <- t(as.matrix(train_dataset[,7:22]))
prediction <- theta %*% input_data
se <- sum((train_dataset$motor_UPDRS - prediction) ^ 2)
return (-(((n/2)*log(2*pi*(sigma^2)))+((1/(2*(sigma^2)))*se)))
}
Ridge <- function(params, lambda){
sigma <- params[1]
theta <- params[-1]
negativemodel_loglikelihood <- -(loglikelihood(sigma, theta))
return (negativemodel_loglikelihood+(lambda * (sum(theta^2))))
}
RidgeOpt <- function(lambda, thetha, sigma)
{
optimization_variables <- c(sigma, thetha)
optim_par <- optim(par = optimization_variables,
fn = Ridge,
lambda =lambda,
method="BFGS")
return (optim_par$par[-1])
}
DF <- function(lambda){
x <- as.matrix(train_dataset[,7:22])
P <- x%*%(solve((t(x) %*% x)+lambda))%*%t(x)
print(sum(diag(P)))
}
intial_weights = coefficients(linearRegression_model)
initial_dispersion = sd(residuals(linearRegression_model))
optimisedWeight_lambda1 = t(as.matrix(RidgeOpt(1, intial_weights, initial_dispersion)))
optimisedWeight_lambda100 = t(as.matrix(RidgeOpt(100, intial_weights, initial_dispersion)))
optimisedWeight_lambda1000 = t(as.matrix(RidgeOpt(1000, intial_weights, initial_dispersion)))
train_data_martix = t(as.matrix(train_dataset[,7:22]))
test_data_matrix = t(as.matrix(test_dataset[,7:22]))
predictions_lambda1_traindata = (optimisedWeight_lambda1 %*% train_data_martix)
predictions_lambda1_testdata = (optimisedWeight_lambda1 %*% test_data_matrix)
predictions_lambda100_traindata = (optimisedWeight_lambda100 %*% train_data_martix)
predictions_lambda100_testdata = (optimisedWeight_lambda100 %*% test_data_matrix)
predictions_lambda1000_traindata = (optimisedWeight_lambda1000 %*% train_data_martix)
predictions_lambda1000_testdata = (optimisedWeight_lambda1000 %*% test_data_matrix)
trainingMse_lambda1 = mean((train_dataset$motor_UPDRS - predictions_lambda1_traindata) ^ 2)
testMse_lambda1 = mean((test_dataset$motor_UPDRS - predictions_lambda1_testdata) ^ 2)
trainingMse_lambda100 = mean((train_dataset$motor_UPDRS - predictions_lambda100_traindata) ^ 2)
testMse_lambda100 = mean((test_dataset$motor_UPDRS - predictions_lambda100_testdata) ^ 2)
trainingMse_lambda1000 = mean((train_dataset$motor_UPDRS - predictions_lambda1000_traindata) ^ 2)
testMse_lambda1000 = mean((test_dataset$motor_UPDRS - predictions_lambda1000_testdata) ^ 2)
print(train_mse)
print(test_mse)
print(trainingMse_lambda1)
print(testMse_lambda1)
DF(1)
print(trainingMse_lambda100)
print(testMse_lambda100)
DF(100)
print(trainingMse_lambda1000)
print(testMse_lambda1000)
DF(1000)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Logistic Regression.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Downloads/ml1.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
setwd("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1")
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
setwd("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1")
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/k nearest neighbour.R", echo=TRUE)
# fitting model based on training and testing model with training data itself and calculating misclassifications rate of training data
target_category <- as.factor(optdigits[train.rows, 65])
# fitting model based on training and testing model with training data itself and calculating misclassifications rate of training data
target_category <- as.factor(optdigits[train.rows, 65])
train_model <- kknn(target_category~., train = train.data, test = train.data, k = 30, kernel = "rectangular")
train_pred <- as.integer(predict(train_model)+0.5) # round off to 0 or 1predict(train_pred)
train_pred <- as.integer(predict(train_model)) # round off to 0 or 1predict(train_pred)
train_pred
train_confusionmatrix
train_misclassification_error = missclass(train.data[,65], train_pred)
train_misclassification_error
train_confusionmatrix
train_confusionmatrix <- table(train.data[,65], train_pred)
train_confusionmatrix
train_misclassification_error = missclass(train.data[,65], train_pred)
train_misclassification_error
# fitting model based on training and testing model with test data itself and calculating misclassifications rate of test data
target_category <- optdigits[train.rows, 65]
# fitting model based on training and testing model with test data itself and calculating misclassifications rate of test data
target_category <- as.factor(optdigits[train.rows, 65])
test_model <- kknn(target_category~., train = train.data, test = test.data, k = 30, kernel = "rectangular")
test_pred <- as.integer(predict(test_model)+0.5) # round off to 0 or 1predict(test_pred)
test_pred <- as.integer(predict(test_model)) # round off to 0 or 1predict(test_pred)
test_confusionmatrix <- table(test.data[,65], test_pred)
test_confusionmatrix
# fitting model based on training and testing model with training data itself and calculating misclassifications rate of training data
target_category <- as.factor(optdigits[train.rows, 65])
train_model <- kknn(target_category~., train = train.data, test = train.data, k = 30, kernel = "rectangular")
train_pred <- as.integer(predict(train_model)) # round off to 0 or 1predict(train_pred)
train_confusionmatrix <- table(train.data[,65], train_pred)
train_confusionmatrix
train_misclassification_error = missclass(train.data[,65], train_pred)
train_misclassification_error
# fitting model based on training and testing model with test data itself and calculating misclassifications rate of test data
target_category <- as.factor(optdigits[train.rows, 65])
test_model <- kknn(target_category~., train = train.data, test = test.data, k = 30, kernel = "rectangular")
test_pred <- as.integer(predict(test_model)) # round off to 0 or 1predict(test_pred)
test_confusionmatrix <- table(test.data[,65], test_pred)
test_confusionmatrix
test_misclassification_error = missclass(test.data[,65], test_pred)
test_misclassification_error
test_pred
View(fitKnn_train)
View(fitKnn_train)
# Task 3
# Finding easy prediction with with probability same as 8
easy_perdiction_8 = which(predict(train_model)==8)
# Finding hard prediction with with highest and lowest prediction of 8
# i.e prediction having values just greater than 7.5 and predictions having probablity just less than 8.5
hard_perdictions_min_8 = which(predict(train_model)==min(predict(train_model)[predict(train_model)>7.5]))
hard_perdictions_max_8 = which(predict(train_model)==max(predict(train_model)[predict(train_model)<8.5]))
# fitting model based on training and testing model with training data itself and calculating misclassifications rate of training data
target_category <- (optdigits[train.rows, 65])
train_model <- kknn(target_category~., train = train.data, test = train.data, k = 30, kernel = "rectangular")
train_pred <- as.integer(predict(train_model)) # round off to 0 or 1predict(train_pred)
train_confusionmatrix <- table(train.data[,65], train_pred)
train_confusionmatrix
train_misclassification_error = missclass(train.data[,65], train_pred)
train_misclassification_error
# fitting model based on training and testing model with test data itself and calculating misclassifications rate of test data
target_category <- (optdigits[train.rows, 65])
test_model <- kknn(target_category~., train = train.data, test = test.data, k = 30, kernel = "rectangular")
test_pred <- as.integer(predict(test_model)) # round off to 0 or 1predict(test_pred)
test_confusionmatrix <- table(test.data[,65], test_pred)
test_confusionmatrix
test_misclassification_error = missclass(test.data[,65], test_pred)
test_misclassification_error
# Task 3
# Finding easy prediction with with probability same as 8
easy_perdiction_8 = which(predict(train_model)==8)
# Finding hard prediction with with highest and lowest prediction of 8
# i.e prediction having values just greater than 7.5 and predictions having probablity just less than 8.5
hard_perdictions_min_8 = which(predict(train_model)==min(predict(train_model)[predict(train_model)>7.5]))
hard_perdictions_max_8 = which(predict(train_model)==max(predict(train_model)[predict(train_model)<8.5]))
# displaying images as heat map
imageVector = as.matrix(train.data[54,1:64])
imageMatrix = matrix(imageVector,8,8,byrow = TRUE)
heatmap(imageMatrix, Rowv = NA, Colv = NA, scale = "row")
# Fitting kknn model and predicting it based on validation data and calculating valdation data misclassification error
target_category <- optdigits[train.rows, 65]
valid_model <- kknn(target_category~., train = train.data, test = valid.data, k = 30, kernel = "rectangular")
valid_pred <- as.integer(predict(valid_model)+0.5) # round off to 0 or 1predict(train_pred)
valid_confusionmatrix <- table(valid.data[,65], valid_pred)
valid_confusionmatrix
valid_misclassification_error = missclass(valid.data[,65], valid_pred)
valid_misclassification_error
# Task 4
# Finding training data misclassifcation error for models with k = 1,...30
k_missclassifcationError_train = list()
for (i in 1:30) {
train_model_k <- kknn(target_category~., train = train.data, test = train.data, k = i, kernel = "rectangular")
train_predict_k <- as.integer(predict(train_model_k)+0.5)
k_missclassifcationError_train[i] <- missclass(train.data[,65], train_predict_k)
}
# Finding validation data misclassifcation error and cross entropy for models with k = 1,...30
k_missclassifcationError_valid = list()
k_crossEntropy_valid = list()
for (i in 1:30) {
valid_model_k <- kknn(target_category~., train = train.data, test = valid.data, k = i, kernel = "rectangular")
valid_predict_k <- as.integer(predict(valid_model_k)+0.5)
valid_predict_prob_k = predict(valid_model_k)
k_missclassifcationError_valid[i] <- missclass(valid.data[,65], valid_predict_k)
# Task 5
k_crossEntropy_valid[i] = -sum(valid.data[,65]*log(valid_predict_prob_k + 10^(-15)))
}
# Task 4
# plotting train and validation misclassification error vs value of K graph
plot(x = c(1:30), y=k_missclassifcationError_train,  xlab = "K", ylab = "Error", type = "b", col ="red")
lines(x = c(1:30), y=k_missclassifcationError_valid,  xlab = "K", ylab = "Error", type = "b", col ="blue")
legend(1,0.18 , legend=c("Training Misclassification error", "Test Misclassification error"), col=c("red", "blue"), lty=1:1, cex=0.8)
# Task 5
# plotting valdaition data cross entropy error vs value of K graph
plot(x = c(1:30), y=k_crossEntropy_valid,  xlab = "K", ylab = "Cross Entropy", type = "b", col ="green")
# fitting model based on training and testing model with training data itself and calculating misclassifications rate of training data
target_category <- as.factor(optdigits[train.rows, 65])
train_model <- kknn(target_category~., train = train.data, test = train.data, k = 30, kernel = "rectangular")
train_pred <- as.integer(predict(train_model)) # round off to 0 or 1predict(train_pred)
train_confusionmatrix <- table(train.data[,65], train_pred)
train_confusionmatrix
train_misclassification_error = missclass(train.data[,65], train_pred)
train_misclassification_error
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 1/Linear_Regression.R", echo=TRUE)
train_pred <- as.integer(predict(train_model, type = "responce"))
train_pred <- as.integer(predict(train_model, type = "response"))
train_pred <- as.integer(predict(train_model, type = "response"))
#Task 1
# Reading the dataset and plotting the scatter plot based in on age and glucose concentration
DF=read.csv("pima-indians-diabetes.csv")
plot(DF[,2], DF[,8], main="Scatterplot",xlab="Glucose Concentration ", ylab="Age", col= ifelse(DF[,9]==1, 'red', 'green'))
legend("topleft",fill=c("red","green"), legend=c("diabetic","non diabetic"),bty="n")
#Task 2
# Fitting the logistic regression model using glm and plotting the scatter plot based in the predictions of the model
logistic <- glm(DF[,9] ~ DF[,2]+DF[,8], data=DF, family="binomial")
summary(logistic)
plot(DF[,2], DF[,8], main="Scatterplot", xlab="Glucose Concentration ", ylab="Age", col= ifelse(predict(logistic, type = "response")>0.5, 'red', 'green'))
legend("topleft",fill=c("red","green"), legend=c("diabetic","non diabetic"),bty="n")
missclass=function(X,X1)
{
n=length(X)
return(1-sum(diag(table(X,X1)))/n)
}
# Finding misclassifcation error of model
olddata <- predict(logistic, type = "response")
a <- predict(logistic, type = "response")
a <- ifelse(a>0.5, 1, 0)
resulterror <- missclass(a, DF[,9])
print(resulterror)
print(summary(logistic))
# Task 3
# Plotting the decision boundary
tetha = coefficients(logistic)
intercept = -(tetha[1]/tetha[3])
slope = -(tetha[2]/tetha[3])
abline(a=intercept, b = slope, col="blue")
legend("topleft",fill=c("red","green"), legend=c("diabetic","non diabetic"),bty="n")
# task 4
# plotting scatterplots with r = 0.2 and r = 0.8
a <- predict(logistic, type = "response")
a <- ifelse(a>0.2, 1, 0)
plot(DF[,2], DF[,8], main="Scatterplot", xlab="Glucose Concentration ", ylab="Age", col= ifelse(predict(logistic, type = "response")>0.2, 'red', 'green'))
plot(DF[,2], DF[,8], main="Scatterplot", xlab="Glucose Concentration ", ylab="Age", col= ifelse(predict(logistic, type = "response")>0.8, 'red', 'green'))
legend("topleft",fill=c("red","green"), legend=c("diabetic","non diabetic"),bty="n")
legend("topleft",fill=c("red","green"), legend=c("diabetic","non diabetic"),bty="n")
# Task 5
# creating new data entires for basis expansion
DF[,10] = DF[,2]^4
DF[,11] = (DF[,2]^3)*DF[,8]
DF[,12] = (DF[,2]^2)*(DF[,8]^2)
DF[,13] = DF[,2]*(DF[,8]^3)
DF[,14] = DF[,2]^4
# fitting new model with added paramters form basis expansion
log_basisexpansion <- glm(DF[,9] ~ DF[,2]+DF[,8]+DF[,10]+DF[,11]+DF[,12]+DF[,13]+DF[,14], data=DF, family="binomial")
summary(log_basisexpansion)
bas <- predict(log_basisexpansion, type = "response")
bas <- ifelse(bas>0.5, 1, 0)
# plotting scatter plot based in new values predicted from newly fiited basis expansion model
plot(DF[,2], DF[,8], main="Scatterplot", xlab="Glucose Concentration ", ylab="Age", col= ifelse(predict(log_basisexpansion, type = "response")>0.5, 'red', 'green'))
legend("topleft",fill=c("red","green"), legend=c("diabetic","non diabetic"),bty="n")
basisexpansionerror <- missclass(bas, DF[,9])
print(basisexpansionerror)
print(resulterror)
setwd("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 2")
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 2/PCA.r", echo=TRUE)
View(pca_res)
# Task 2
pca_res = princomp(scaled_dataset)
summary(pca_res)
pca_scores = as.data.frame.matrix(pca_res$scores)
# plotting PC1 Scores
ggplot(data = pca_scores, aes(x = Comp.1, y = 0)) +
geom_point(alpha = 0.1)
pca_scores = as.data.frame.matrix(pca_res$loadings)
# plotting PC1 Scores
ggplot(data = pca_scores, aes(x = Comp.1, y = 0)) +
geom_point(alpha = 0.1)
# plotting PC1 Scores
plot(pca_scores)
# Task 2
pca_res = princomp(scaled_dataset)
summary(pca_res)
pca_scores = as.data.frame.matrix(pca_res$loadings)
# plotting PC1 Scores
ggplot(data = pca_scores, aes(x = Comp.1, y = 0)) +
geom_point(alpha = 0.1)
# Finding features in PC1 that contribute more than 0.1
pca_eigen_vector = as.data.frame.matrix(pca_res$loadings)
pc1 = as.data.frame(pca_eigen_vector[,1] , row.names = row.names(pca_eigen_vector))
row.names(pc1)[which(abs(pc1[,1])>0.1)]
# Finding five feature that contribute most in PC1 and printing their values
highest_contributors_pc1 = tail(sort(unlist(abs(pc1), use.names = FALSE)), 5)
row.names(pc1)[which(abs(pc1[,1]) %in% highest_contributors_pc1)]
unlist(pc1, use.names = FALSE)[which(abs(pc1[,1]) %in% highest_contributors_pc1)]
# Finding five feature that contribute most in PC2
pc2 = as.data.frame(pca_eigen_vector[,2] , row.names = row.names(pca_eigen_vector))
highest_contributors_pc2 = tail(sort(unlist(abs(pc2), use.names = FALSE)), 6)
row.names(pc2)[which(abs(pc2[,1]) %in% highest_contributors_pc2)]
unlist(pc2, use.names = FALSE)[which(abs(pc2[,1]) %in% highest_contributors_pc2)]
# Plotting PC scores in the coordiantes of PC1 and PC2
pca_scores$ViolentCrimesPerPop <- dataset_communities$ViolentCrimesPerPop
ggplot(data = pca_scores, aes(x = Comp.1, y = Comp.2, color = ViolentCrimesPerPop)) +
geom_point(aes(alpha = 1)) +
scale_color_gradient(low="green", high="red")
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 2/PCA.r", echo=TRUE)
View(pca_scores)
View(pca_res)
source("~/Desktop/Masters - LIU/Course/Sem 1/Period 2/TDDE01 - Machine Learning/Labs/Lab 3/neuralNetworks.R", echo=TRUE)
# Task 3
var <- runif(500, 0, 50)
newdata_test <- data.frame(var, Sin=sin(var))
newprediction <- predict(nn, newdata=newdata_test)
plot(newdata_test[,1], newdata_test[,2], ylim=c(-10,1), xlab="Sample Value", ylab = "Sine of Sample Values")
points(newdata_test[,1], newprediction, col = "blue")
legend("bottomleft",fill=c("black","blue"), legend=c("Actual Test data point","Prediction Data Points"),bty="n")
sum(nn$weights)
sum(as.vector(nn$weights))
sum(unlist(nn$weights))
loadings
nn$weights
unlist(nn$weights)
